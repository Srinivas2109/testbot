{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srinivas2109/testbot/blob/main/gemma_connection_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X3buE07zAfO"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBJVvxlQzAfP"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers -U\n",
        "!pip install accelerate\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install trl\n",
        "!pip install git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj0cKjK7zAfP"
      },
      "outputs": [],
      "source": [
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MLU53-tzAfP"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login(write_permission=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHaQ5JJAzAfQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Define conversation template\n",
        "chat_template = \"\"\"\n",
        "**You:** {user_input}\n",
        "**Gemma-7b:** {model_response}\n",
        "\"\"\"\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "#model_id = \"google/gemma-2b-it\"\n",
        "#lora_config = BitsAndBytesConfig(\n",
        "#    load_in_4bit=True,\n",
        " #   bnb_4bit_quant_type=\"nf4\",\n",
        "  #  bnb_4bit_compute_dtype=torch.bfloat16\n",
        "#)\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", quantization_config=quantization_config)\n",
        "\n",
        "\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# quantized_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=lora_config, device_map={\"\":0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I8EtaZKazAfQ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def chatbot():\n",
        "  \"\"\"\n",
        "  Interactive chat function with Gemma-2b chatbot.\n",
        "  \"\"\"\n",
        "  while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    # Encode user input with tokenizer\n",
        "   # input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate model response\n",
        "\n",
        "    input_ids = tokenizer(user_input, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**input_ids)\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "    # output = quantized_model.generate(input_ids, max_length=1024, do_sample=True, top_k=10, top_p=0.9)\n",
        "    # model_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print conversation\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to the Gemma-7b Chatbot!\")\n",
        "    chatbot()\n",
        "    print(\"Goodbye!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8xUwWoJzAfQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbPKKY9kzAfQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETZy7XDWzAfQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30674,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}